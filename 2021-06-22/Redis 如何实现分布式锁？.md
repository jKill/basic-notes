## Redis 如何实现分布式锁？
### RedLock流程

- 1、获取当前时间
- 2.1、RedLock会向每个master申请锁，只有在多数实例（N/2+1，如5个master中的3个）上获得锁，并且获取锁花费的总时间没有超过lock validity time时，才算是成功获得一个分布式锁。
- 2.2、如果没成功获得锁（比如没能锁住N/2+1的实例，或者validity time是负的），不管实例有没有锁住，它会尝试解锁所有的实例。
- 3、再次获取当前时间
- 4、检查获取锁的过程是否超时
- 5、执行锁保护的业务逻辑

### RedLock如何保证不被其他客户端解锁（删掉它set的key）
客户端获得锁后会设置一个随机字符串（来自/dev/urandom的20个字节）作为value，只有产生这个字符串的客户端，才能删除锁对应的key。
### RedLock如何避免客户端挂掉而长时间占着锁
RedLock有自动释放的时间（lock validity time），到时间锁对应的key会被删除。
### 如何避免网络延迟或者进程停顿（如GC），导致获取到锁时key已经过期
RedLock获取锁也有一个超时时间，并且这个时间比RedLock自动释放的时间短。
### RedLock的保证时间
只有当客户端在lock validity time-几毫秒（补偿处理器之间的时间误差）的时间内，完成它的工作，RedLock才能保证安全互斥。

### Retry on failure：

- 如果一个客户端没能获得锁，它应该在一个随机延迟后，再次尝试获取锁，以避免出现多个客户端同时尝试获取锁的情况（可能会导致脑裂）。
- 如果没能成功获取RedLock（在N/2+1的实例上成功set），应该尽快将这次获取RedLock所set的key删除掉，以便再次获取锁

如果没能在锁自动释放前锁住大部分(N/2+1)节点，会依次删除之前set的key。

### 系统的活性(liveness)基于以下三点

- 锁的自动释放（key会过期）
- 当获取锁失败，或者获得锁并完成了工作后，客户端会合作移除锁对应的key，使得再次获取锁时，不需要等待key过期。
- 当客户端需要重新尝试获取锁时，它需要等待的时间必须比获得大多数（N/2+1）节点锁的时间大，以避免在争抢资源的时候脑裂。（TODO：正确翻译并理解）

### 崩溃恢复（crash-recovery）
- 在没有持久化的情况下，集群中5个节点，其中一个客户端获得3个节点的锁，成功获得RedLock。但是3个节点中的一个重启了，这时候集群中又有了3个节点可以获得锁。
- 在开启了AOF持久化的情况下，如果是正常关闭（发送SHUTDOWN并重启）是没问题的。但如果是断电等情况，由于AOF默认每秒fsync一次，我们有可能丢失一秒内set的key。如果设置每次操作都fsync一次，又会导致性能完全比不上同层次的CP分布式锁系统。
- 算法的安全性，在节点重启后，不参与当前活跃（currently active）的锁获取时，基本上可以得到保证。在实现上，主要通过延迟重启(delayed restarts)。在现存的所有锁相关的key过期并自动释放，再重启。
- 缺陷：牺牲可用性。如果大部分节点重启，会导致整个系统不可用。

### 使算法更可靠
RedLock可以使用一个小的自动释放时间作为默认lock validity times。具体操作为：如果在客户端的计算过程中，锁的有效时间已经快要到了，客户端可以通过发送一个Lua脚本到所有服务端实例，延长锁对应key的TTL（这里需要验证key存在并且value还是获取锁时**本客户端**分配的随机值）

## Martin与Redis作者关于RedLock的辩论
### Martin Kleppmann论文对RedLock的看法

- Redis不适合在需要**强一致性**和**持久化**的场景下使用，并且分布式锁也属于这种场景。
- 使用锁的目的包括效率和正确性。
- 效率：使用RedLock需要5个节点，并且引入复杂性和成本。如只是为了效率，还不如使用单个Redis节点。
- 正确性：假设一个用分布式锁保护更新文件的场景。如果客户端1获取到了锁，但在写回数据到存储介质前，发生了STW(stop-the-world) GC。GC期间锁自动释放了，客户端2获取到了锁并更新了同一份数据。这时客户端1的GC终于结束了，然而它没意识到自己的锁已经超时失效，还是把数据写回了存储介质。导致数据的不安全更新。
![](https://martin.kleppmann.com/2016/02/unsafe-lock.png)

- 这个bug并不仅仅在理论上存在，Hbase曾经也有这个问题。因为STW比普通GC长得多——可达几分钟，超过了大多数分布式锁的有效时间。
- 在写回更新数据之前检查锁的失效时间也没用，因为GC能在任何时间点停止一个运行中的线程。在最后一次检查和写操作之间，发生了GC，那么这个检查就无效了。
- 就算没有长时间的GC停顿，你的**进程也会因为各种原因停顿**。如读取了没加载进内存的地址，因此发生缺页异常，等待这一页从磁盘加载回来；或者说你用的是EBS这种云硬盘，需要等待网络延迟；又或者你的进程收到了SIGSTOP信号。
- 解决方案是增加fencing token，在客户端1获取到锁时，也同时收到一个token=33，然后同样在写回数据前发生了STW，期间客户端2拿到了锁和token=34，更新并写回数据。此时STW结束，客户端1的更新也写回存储介质了，此时锁服务已经处理过值为34的token了，34比33大，所以客户端1的写回会被拒绝。
![](https://martin.kleppmann.com/2016/02/fencing-tokens.png)

- fencing token的核心是要维护一个全局严格递增的token，因此可以使用ZooKeeper作为锁服务，使用zxid或者znode的版本号作为fencing token。
- RedLock使用timeout作为错误检测手段，存在以下问题：1、请求超时不一定是节点宕机了，也可能是**网络延迟**；2、也可能是时钟错误（**时钟跳跃**）。
- Redis使用```gettimeofday```计算超时时间，而不是``` monotonic clock```。```gettimeofday```使用手册明确说明了这个方法的返回，会受到系统时钟跳跃(discontinuous jumps in system time)的影响，导致提前或者延后key超时的发生。	
- 假设有A，B，C，D，E 五个Redis节点，客户端1和客户端2在申请同一个RedLock。如果客户端1获得了A，B，C 三个节点的锁，但是C的时钟向前跳跃，导致C的key提前过期，锁自动释放了，这时客户端2就可以获得C，D，E三个节点的锁。这样一来，客户端1、2都认为自己持有锁。
- C在锁的key持久化之前宕机，并马上重启也会导致上述问题。RedLock文档推荐延迟节点的重启时间，至少延迟存活时间最长的锁的一个TTL。但这个重启延迟同样会受时钟跳跃的影响。
- 结论：RedLock不是一个好选择，因为是一个四不像（neither fish nor fowl）。就效率而言，多个节点带来的复杂性和成本不如单节点的Redis锁；就安全性而言，RedLock基于网络延迟、进程停顿以及时钟跳跃都有限的假设上。但实际上，这三个因素影响很可能会很大，导致RedLock无法保证互斥。

### antirz对Martin Kleppmann回应
#### 对fencing token方案的回应：
- 1、大多数时候需要使用分布式锁的时候，都是系统里缺少解决资源竞争的手段。如果你已经有了另一种解决资源竞争的方法，那么就没必要再使用分布式锁。
- 2、Martin提到的GC停顿，在使用fencing token时也会发生，因此获取锁的顺序和把数据写回共享资源的顺序不能保证是一致的。

Martin主要指出三个问题会导致RedLock不安全：

- 时钟跳跃
- 进程停顿
- 网络延迟

#### 对时钟跳跃问题的回应：
在NTP配置正确的情况下，时钟跳跃并不是实际存在的问题。
#### 对网络延迟问题的回应：
Martin举了两个客户端向5个节点获取锁，其中一个节点因为网络延迟，导致最终两个人节点同时拿到了锁，但RedLock获取锁的步骤是这样的：

- 1、获取当前时间
- 2、进行获取锁必须的步骤
- 3、再次获取当前时间
- 4、检查获取锁的过程是否超时
- 5、执行锁保护的业务逻辑

如果发生在前3个步骤的网络延迟很大，会导致步骤4的检查结果为”获取锁超时“，不会误以为自己拿到了锁。而在后面的步骤发生的延迟，可以归类到”客户端拿到了锁，但没有在key过期前完成工作“这一类问题。
#### 对进程停顿问题的回应：
分析同网络延迟。Martin提出的case同样可以归类为”客户端拿到了锁，但没有在key过期前完成工作“
#### 题外话
在避免进程长时间休眠这件事上，可以做的有很多；但是要避免网络延迟，能做的相对少很多。
#### 结论

- 除了系统时钟被修改这一问题，antirez认为Mathin提出的其他问题都不能成为不使用RedLock的原因。
- 在系统时钟的问题上，他同意使用递增的fencing token作为校验方法。